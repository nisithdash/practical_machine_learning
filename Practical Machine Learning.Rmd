---
title: "Machine Learning Project"
author: "Nisith Dash"
date: "Saturday, August 22, 2015"
output:
  html_document:
    keep_md: yes
---

### Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

### Data

The training data for this project are available here: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv]

The test data are available here: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv]

The data for this project come from this source: [http://groupware.les.inf.puc-rio.br/har] (Groupware@LES: group of research and development of groupware technologies.).

### Goal

The goal is to find a model that predicts the classes below based on activity data.

- exactly according to the specification (Class A)
- throwing the elbows to the front (Class B)
- lifting the dumbbell only halfway (Class C)
- lowering the dumbbell only halfway (Class D)
- throwing the hips to the front (Class E)

## Loading data

First, download and load the datasets. Below the code for loading the data 
```{r echo=TRUE, warning=FALSE, message=FALSE}

library("knitr")
library("dplyr")
library("caret")
library("tidyr")
library(rpart) 
library(rpart.plot)

download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv','pml-training.csv','curl')

set.seed(54356)
opts_chunk$set(fig.path = "./figures/") # Set figures path

# Some missing values are coded as string "#DIV/0!" or "." or "NA" - these will be changed to NA.
pml.training <- read.csv("pml-training.csv", na.strings = c("NA","#DIV/0!", ""), dec = ".")

```

## Cleaning data

It is necessary to reduce the noise in the data and remove items that have no relevance from a modeling perspective

1. Remove new_window == yes observations because these seem to be aggregate of other column.
2. Remove all columns with NA values.
3.Some variables are irrelevant and can be deleted from the dataset:
user_name, raw_timestamp_part_1, raw_timestamp_part_,2 cvtd_timestamp, new_window, and num_window (columns 1 to 7).

```{r}
x <- pml.training %>% filter(new_window == "no")
x <- x[ , ! apply(x ,2 ,function(x) any(is.na(x)) ) ]
x <- x[8:length(x)]
```

### Data Partitioning

The training data needs to be partioned to allow cross-validation. Training data set is partionned into 2 sets: trainingset (75%) and testset (25%). This will be performed using random subsampling without replacement.

```{r}
inTrain <- createDataPartition(y=x$classe, p=0.75, list=FALSE)
trainingset <- subset(x[inTrain,])
testset <- subset(x[-inTrain,])
```

### Introspect Data

The variable “classe” contains 5 levels: A, B, C, D and E. A plot of the outcome variable vs frequency of each levels in the training data set and compare one another.

```{r}
plot(trainingset$classe, main="Level of the variable classe within the training data set", xlab="classe", ylab="freq")
```

It is evident that Level A is the most frequent with more than 4000 occurrences while level D is the least frequent with about 2500 occurrences.

## Prediction Models

### Decision Tree Model

```{r warning=FALSE, message=FALSE}
Ctrl <- trainControl(method = "repeatedcv", repeats = 3)
model_rpart <- train(classe ~ roll_belt + pitch_forearm + yaw_belt + roll_forearm + magnet_dumbbell_z + 
                             pitch_belt + magnet_dumbbell_y + magnet_dumbbell_x + accel_belt_z + 
                             magnet_belt_z, data=trainingset, method="rpart", tuneLength = 30, trControl = Ctrl)
predictions_rparttest <- predict(model_rpart, testset)


# Test results on our testing data set:
confusionMatrix(predictions_rparttest, testset$classe)
```

### Random forest model 

```{r warning=FALSE, message=FALSE}
Ctrl <- trainControl(method = "oob")
model_rf <- train(classe ~ roll_belt + pitch_forearm + yaw_belt + roll_forearm + magnet_dumbbell_z + 
                             pitch_belt + magnet_dumbbell_y + magnet_dumbbell_x + accel_belt_z + 
                             magnet_belt_z, data=trainingset, method="rf", 
                  trControl = Ctrl, tuneGrid = data.frame(.mtry = 2))
model_rf

predictions_rftest <- predict(model_rf, testset)
confusionMatrix(predictions_rftest, testset$classe)
```

### In Sample/Out of Sample Error of Random Forest model

```{r}
model_rf$finalModel ## in-sample
confusionMatrix(predictions_rftest, testset$classe) ## out-of-sample
```

## Conclusion
Random Forest performs better than Decision Trees.

Accuracy for Random Forest model was 0.9852 (95% CI: (0.9814, 0.9884)) compared to 0.8132 (95% CI: (0.8019, 0.8241)) for Decision Tree model. The Random Forest model is choosen for predicting test data.

###Prediction on the test data
We concluded that Random Forest model is the best model to predict. Let's try it on test data

```{r fig.width=9, fig.height=3}

bestfit <- model_rf

download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv','pml-testing.csv','curl')

pml.submission <- read.csv("pml-testing.csv", na.strings = c("NA","#DIV/0!", ""), dec = ".")

predprob <- predict(bestfit, pml.submission, type = "prob")
predprob$testcase <- 1:nrow(predprob)
predprob <- gather(predprob, "class", "prob", 1:5)
ggplot(predprob, aes(testcase, class)) + 
        geom_tile(aes(fill = prob), colour = "white") + 
        geom_text(aes(fill = prob, label = round(prob, 2)), size=3, colour="grey25") +
        scale_fill_gradient(low = "white", high = "green") +
        scale_x_discrete(expand = c(0, 0)) +
        scale_y_discrete(expand = c(0, 0)) 

final_predictions <- predict(bestfit, pml.submission)
final_predictions
```

### Write files for submission
```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(final_predictions)
```

#### References
- Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

